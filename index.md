---
layout: page
---

# About Me

<!-- <img src="https://caihanlin.com/caihanlin.jpg" class="floatpic" width="360" height="480">

Here is **Hanlin Cai (Lance, [蔡汉霖](https://caihanlin.com/file/蔡汉霖简历.pdf))**.

I am a senior student majoring in **Automation** at Fuzhou University and **Robotics** at Maynooth University (Ireland, Combined Degrees). Currently, I am working as a research assistant in the **IACTIP Lab** (Provincial Key), advised by [Prof. Zhezhuang Xu](https://www.researchgate.net/profile/Zhezhuang-Xu). Here is [my CV](https://caihanlin.com/file/CV-HanlinCAI.pdf). -->


<!-- <img src="https://caihanlin.com/caihanlin.jpg" class="floatpic" width="360" height="480"> -->

Here is **Shitou Zhang (Patrick, 张石头)**.

<!-- I am a senior student majoring in **Automation** at Fuzhou University and **Robotics** at Maynooth University (Ireland, Combined Degrees). Currently, I am working as a research assistant in the **IACTIP Lab** (Provincial Key), advised by [Prof. Zhezhuang Xu](https://www.researchgate.net/profile/Zhezhuang-Xu). Here is [my CV](https://caihanlin.com/file/CV-HanlinCAI.pdf). -->

<!-- I am a research assistant at Wuhan University. I received my BSc. Degree from School of Information Management, Wuhan University and my MSc. Degree from Department of Information Engineering, the Chinese University of Hong Kong. I am a member of the Key Laboratory of Smart Archives (Wuhan) under the leadership of [Prof. Ping Wang](https://sim.whu.edu.cn/info/1571/85472.htm). Additionally, I'm fortunate to receive guidance from [Dr. Zuchao Li](https://zcli-charlie.github.io/) and [Prof. John K. Zao](https://www.ie.cuhk.edu.hk/faculty/zao-kar-kin-john/). -->

I am a passionate research intern at [DeepLang](https://deeplang.ai/), where I focus on LLM pretraining. I also work on domain-specific LLM development as a member of the Key Laboratory of Archival Intelligent Development and Service, NAAC. I am privileged to work under the guidance of  [Prof. John K. Zao](https://www.ie.cuhk.edu.hk/faculty/zao-kar-kin-john/), [Dr. Zuchao Li](https://zcli-charlie.github.io/), and [Prof. Ping Wang](https://sim.whu.edu.cn/info/1571/85472.htm). My recent projects involve investigating the scaling law of decoder-only MoE models and exploring multi-task generalization through PEFT.

<br>

---

## Education Background

<!-- **<font color='red'>[Highlight]</font> I am looking for PhD to start in 2025 Fall. Contact me if you have any leads!** -->

<!-- - **Sep. 2022 - Jun. 2023:** the Chinese University of Hong Kong (MSc)
- **Sep. 2018 - Jun. 2022:** Wuhan University (BSc) -->

- **Sep. 2022 - Jun. 2023:** Department of Information Engineering, the Chinese University of Hong Kong (MSc)
- **Sep. 2018 - Jun. 2022:** School of Information Management, Wuhan University (BSc)

<br>

---

## Research Interests

- Mixture of Experts (MoE)
- Multimodal Pretraining

> "Trainging language models is to achieve scaling and avoid bottlenecks." -- Jared Kaplan

<!-- My research interest centers around **modularization** and **micro-servitization** of large-scale AI systems, where various utilities can be achieved through decomposing and recomposing of modules. By encapsulating real-world information, knowledge, expertise, and experience into neural-based dense representations and sub-networks, backbone model and use-case-specific properties can be decoupled, facilitating more use-case-centric and computation-efficient solutions through the synergy of modules. Such architectural design enhances compatibility with distributed learning, providing augmented security essential for high-staking applications, such as neuroscience-driven human-AI interactions. -->


My research interest is centered around the scaling of LLMs. The fundamental intelligence of LLMs is obtained from the pretraining stage, where exponential scaling leads to a linear reduction in test loss, as revealed by the scaling law. This has spurred my interest in exploring efficient scaling methods, such as MoE and model merging. Additionally, I am captivated by the concept of multimodal pretraining, which can be viewed as scaling across various modal dimensions.


<br>

---

## News and Updates
- **Nov 2023：** ArcMMLU paper is available on [arXiv](https://arxiv.org/abs/2311.18658), check out the [introduction video](https://www.bilibili.com/video/BV1MG411v7tJ) produced by PaperWeekly.
- **Nov 2023：** Our ArcMMLU dataset is open-sourced, available on [Github](https://github.com/stzhang-patrick/ArcMMLU) and [HuggingFace](https://huggingface.co/datasets/patrickshitou/ArcMMLU)
- **Nov 2023：** Our [Archives Meet GPT] paper is accepted by iConference 2024.
- **Oct 2023：** Our LingoWhale-8B is open-sourced, available on [Github](https://github.com/DeepLangAI/LingoWhale-8B) and [HuggingFace](https://huggingface.co/deeplang-ai/LingoWhale-8B)
- **Aug 2023：** BATGPT new version is updated on [arXiv](https://arxiv.org/abs/2307.00360)
- **Jul 2023：** Our BATGPT-15B-sirius is open-sourced, available on [GitHub](https://github.com/zcli-charlie/BatGPT) and [HuggingFace](https://huggingface.co/MLP-lab/BatGPT-15B-sirius).
- **Jul 2023：** ArcGPT paper is available on [arXiv](https://arxiv.org/abs/2307.14852).
- **Jul 2023：** BATGPT paper is available on [arXiv](https://arxiv.org/abs/2307.00360).
- **May 2023：** One [paper](https://aclanthology.org/2023.acl-srw.15.pdf) is accepted by ACL SRW 2023.